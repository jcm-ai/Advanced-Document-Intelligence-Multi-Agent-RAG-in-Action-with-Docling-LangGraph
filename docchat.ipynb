{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Document Intelligence: Multi Agent RAG in Action with Docling LangGraph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever struggled to extract precise information from long, complex documents? Whether itâ€™s a research paper, legal contract, technical report, or environmental study, finding the exact details you need can feel overwhelming. Thatâ€™s where `DocChat` comes inâ€”a `multi-agent RAG` tool designed to help us ask questions about our documents and receive fact-checked, hallucination-free answers.\n",
    "\n",
    "Sure, we could use ChatGPT or DeepSeek to accomplish this task, but when dealing with long documents containing multiple tables, images, and dense text, these models struggle with retrieval and are prone to hallucinations. They often misinterpret tables, miss key data hidden in footnotes, or even fabricate citationsâ€”which I will demonstrate below. The problem? These models lack document-aware reasoning and donâ€™t verify their responses against structured sources.\n",
    "\n",
    "Thatâ€™s why DocChat takes a different approach. Instead of relying on a single LLM, it combines multiple AI agents, each with a specific role:\n",
    "\n",
    "- A `Hybrid Retriever` that intelligently combines BM25 keyword search and **vector embeddings** to retrieve the most relevant passages.\n",
    "- A `Research Agent` that analyzes the retrieved content and generates an initial response.\n",
    "- A `Verification Agent` that cross-checks the response against the original document to detect hallucinations and flag unsupported claims.\n",
    "- A `Self-Correction Mechanism` that re-runs the research step if any contradictions or unsupported statements are found.\n",
    "\n",
    "This multi-step, verification-driven approach ensures that DocChat provides precise, document-grounded answers, even for complex and long-form documents that general-purpose chatbots struggle with. Whether we need to extract specific data points, summarize sections, compare multiple reports, or analyze tables, DocChat is built to help us navigate our documents with confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Why-to-use-multi-agent-RAG\">Why to use multi-agent RAG</a></li>\n",
    "    <li><a href=\"#DocChat-Workflow\">DocChat-Workflow</a></li>\n",
    "    <li><a href=\"#Document-Parsing-with-Docling\">Document Parsing with Docling</a></li>\n",
    "    <li><a href=\"#Build-a-Vector-Database-with-ChromaDB\">Build a Vector Database with ChromaDB</a></li>\n",
    "    <li><a href=\"#Logging-and-Configuration\">Logging and Configuration</a></li>\n",
    "    <li><a href=\"#Document-Preprocessor-Module\">Document Preprocessor Module</a></li>\n",
    "    <li><a href=\"#LangGraph-multi-agent-system-structure\">LangGraph multi-agent system structure</a></li>\n",
    "    <li><a href=\"#Relevance-Checker:-ensuring-query-document-alignment\">Relevance Checker: ensuring query-document alignment</a></li>\n",
    "    <li><a href=\"#Research-Agent:-Generating-Document-Based-Answers\">Research Agent: Generating Document-Based Answers</a></li>\n",
    "    <li><a href=\"#Verification-Agent:-Validating-Answer-Accuracy-and-Relevance\">Verification Agent: Validating Answer Accuracy and Relevance</a></li>\n",
    "    <li><a href=\"#Hybrid-retriever:-combining-BM25-and-vector-search-for-optimal-document-retrieval\">Hybrid retriever: combining BM25 and vector search for optimal document retrieval</a></li>\n",
    "    <li><a href=\"#Agent-Workflow:-Orchestrating-the-Multi-Agent-RAG-System\">Agent Workflow: Orchestrating the Multi-Agent RAG System</a></li>\n",
    "    <li><a href=\"#Main-Process-Function:-Tying-It-All-Together\">Main Process Function: Tying It All Together</a></li>\n",
    "    <li><a href=\"#Example-Usage:-Running-the-RAG-System\">Example Usage: Running the RAG System</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this project, we will be able to:\n",
    "\n",
    "1. **Implement a multi-agent RAG system**: Develop a sophisticated question-answering system that leverages specialized agents for document processing, relevance checking, research, and verification.\n",
    "\n",
    "2. **Create efficient document processing pipelines**: Build a system that processes various document formats (PDF, DOCX, TXT, MD) into searchable chunks with caching for improved performance.\n",
    "\n",
    "3. **Design hybrid retrieval mechanisms**: Implement a combined BM25 and vector search system that balances keyword precision with semantic understanding.\n",
    "\n",
    "4. **Orchestrate complex agent workflows**: Use LangGraph to create state-based workflows that enable sophisticated branching logic and feedback loops.\n",
    "\n",
    "5. **Integrate with IBM WatsonX AI**: Connect to IBM's AI services for embeddings and large language models, enabling advanced text processing capabilities.\n",
    "\n",
    "6. **Implement robust verification systems**: Create agents that fact-check AI-generated responses against source documents to ensure accuracy and reliability.\n",
    "\n",
    "7. **Handle errors gracefully**: Implement comprehensive error handling throughout the system to ensure resilience and helpful messaging.\n",
    "\n",
    "8. **Test your system with real documents**: Evaluate the system with practical document-based queries that demonstrate its ability to extract accurate information from complex content.\n",
    "\n",
    "This project equips us with the skills to create sophisticated document intelligence systems that can accurately answer questions based on document content, making information retrieval more accessible and reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this project, we will be using the following libraries:\n",
    "* `docling` for document extraction and processing from various file formats.\n",
    "* `langchain` for building modular AI applications with retrievers and document loaders.\n",
    "* `langgraph` for creating directed graph workflows with conditional routing for agent coordination.\n",
    "* `langchain_text_splitters` for dividing documents into processable chunks using markdown headers.\n",
    "* `langchain_community` for integrating retrieval systems like BM25 and vectorstores.\n",
    "* `chromadb` for efficient vector storage and retrieval of document embeddings.\n",
    "* `ibm_watsonx_ai` for accessing IBM's foundation models and embedding services.\n",
    "* `rank_bm25` for implementing the BM25 retrieval algorithm.\n",
    "* `pickle` for serializing and deserializing Python objects in the caching system.\n",
    "* `hashlib` for generating hash values for efficient document deduplication and caching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Shapely-2.1.0 XlsxWriter-3.2.3 click-8.2.0 docling-2.30.0 docling-core-2.30.0 docling-ibm-models-3.4.3 docling-parse-4.0.1 easyocr-1.7.2 et-xmlfile-2.0.0 filelock-3.18.0 filetype-1.2.0 fsspec-2025.3.2 hf-xet-1.1.0 huggingface_hub-0.31.1 imageio-2.37.0 jsonlines-3.1.0 jsonref-1.1.0 latex2mathml-3.78.0 lazy-loader-0.4 lxml-5.4.0 markdown-it-py-3.0.0 marko-2.1.3 mdurl-0.1.2 mpire-2.10.2 mpmath-1.3.0 multiprocess-0.70.18 networkx-3.4.2 ninja-1.11.1.4 numpy-2.2.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opencv-python-headless-4.11.0.86 openpyxl-3.1.5 pandas-2.2.3 pillow-11.2.1 pyclipper-1.3.0.post6 pydantic-settings-2.9.1 pylatexenc-2.10 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.1.2 python-dotenv-1.1.0 python-pptx-1.0.2 regex-2024.11.6 rich-14.0.0 rtree-1.4.0 safetensors-0.5.3 scikit-image-0.25.2 scipy-1.15.3 semchunk-2.2.2 shellingham-1.5.4 sympy-1.14.0 tabulate-0.9.0 tifffile-2025.5.10 tokenizers-0.21.1 torch-2.7.0 torchvision-0.22.0 transformers-4.51.3 triton-3.3.0 typer-0.15.3 typing-inspection-0.4.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed config-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed langchain-core-0.3.59 langgraph-0.4.3 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.66 langsmith-0.3.42 orjson-3.10.18 ormsgpack-1.9.1 requests-toolbelt-1.0.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed langchain-0.3.25 langchain-text-splitters-0.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed ibm-cos-sdk-2.14.0 ibm-cos-sdk-core-2.14.0 ibm-cos-sdk-s3transfer-2.14.0 ibm-watsonx-ai-1.3.13 jmespath-1.0.1 langchain_ibm-0.3.10 lomond-0.3.3 requests-2.32.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ibm_watsonx_ai) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chromadb-1.0.8 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.9 fastapi-0.115.9 flatbuffers-25.2.10 google-auth-2.40.1 googleapis-common-protos-1.70.0 grpcio-1.71.0 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.33.0 opentelemetry-exporter-otlp-proto-common-1.33.0 opentelemetry-exporter-otlp-proto-grpc-1.33.0 opentelemetry-instrumentation-0.54b0 opentelemetry-instrumentation-asgi-0.54b0 opentelemetry-instrumentation-fastapi-0.54b0 opentelemetry-proto-1.33.0 opentelemetry-sdk-1.33.0 opentelemetry-semantic-conventions-0.54b0 opentelemetry-util-http-0.54b0 posthog-4.0.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rsa-4.9.1 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 websockets-15.0.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install docling==2.30.0 | tail -n 1\n",
    "%pip install config==0.5.1 | tail -n 1\n",
    "%pip install langgraph | tail -n 1\n",
    "%pip install langchain | tail -n 1\n",
    "%pip install langchain_community | tail -n 1\n",
    "%pip install langchain_ibm | tail -n 1\n",
    "%pip install ibm_watsonx_ai | tail -n 1\n",
    "%pip install chromadb | tail -n 1\n",
    "%pip install rank_bm25 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "Import all required libraries here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, TypedDict, Optional, Union\n",
    "import time\n",
    "import os\n",
    "# Core components\n",
    "from docling.document_converter import DocumentConverter  # For PDF/document parsing\n",
    "from langgraph.graph import StateGraph, END  # For workflow management\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why to use multi-agent RAG\n",
    "\n",
    "A [NaÃ¯ve RAG (Retrieval-Augmented Generation)](https://research.ibm.com/blog/retrieval-augmented-generation-RAG?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-DocChat-v1_1738281360) pipeline is often insufficient for handling long, structured documents due to several limitations:\n",
    "\n",
    "ðŸ”´ Limited query understanding â€“ NaÃ¯ve RAG processes queries at a single level, failing to break down complex questions into multiple reasoning steps. This results in shallow or incomplete answers when dealing with multi-faceted queries.\n",
    "\n",
    "ðŸ”´ No hallucination detection or error handling â€“ Traditional RAG pipelines lack a verification step. This means that if a response contains hallucinated or incorrect information, thereâ€™s no mechanism to detect, correct, or refine the output.\n",
    "\n",
    "ðŸ”´ Inability to handle out-of-scope queries â€“ Without a proper scope-checking mechanism, NaÃ¯ve RAG may attempt to generate answers even when no relevant information exists, leading to misleading or fabricated responses.\n",
    "\n",
    "ðŸ”´ Inefficient multi-document retrieval â€“ When multiple documents are uploaded, a NaÃ¯ve RAG system might retrieve irrelevant or suboptimal passages, failing to select the most relevant content dynamically.\n",
    "\n",
    "To overcome these challenges, DocChat implements a Multi-Agent RAG research system, which introduces intelligent agents to enhance retrieval, reasoning, and verification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How multi-agent RAG solves these issues?\n",
    "\n",
    "âœ… Scope checking & routing\n",
    "- A Scope-Checking Agent first determines whether the userâ€™s question is relevant to the uploaded documents. If the query is out of scope, DocChat explicitly informs the user instead of generating hallucinated responses.\n",
    "\n",
    "âœ… Dynamic multi-step query processing\n",
    "- For complex queries, an Agent Workflow ensures the question is broken into smaller sub-steps, retrieving the necessary information before synthesizing a complete response.\n",
    "- For example, if a question requires comparing two sections of a document, an agent-based approach recognizes this need, retrieves both parts separately, and constructs a comparative analysis in the final answer.\n",
    "\n",
    "âœ… Hybrid retrieval for multi-document contexts\n",
    "- When multiple documents are uploaded, the Hybrid Retriever (BM25 + Vector Search) ensures that the most relevant document(s) are selected dynamically, improving accuracy over traditional retrieval pipelines.\n",
    "\n",
    "âœ… Fact verification & self-correction\n",
    "- After an initial response is generated, a Verification Agent cross-checks the output against the retrieved documents.\n",
    "- If any contradictions or unsupported claims are found, the Self-Correction Mechanism refines the answer before presenting it to the user.\n",
    "\n",
    "âœ… Shared global state for context awareness\n",
    "- The Agent Workflow maintains a shared state, allowing each step (retrieval, reasoning, verification) to reference previous interactions and refine responses dynamically.\n",
    "- This enables context-aware follow-up questions, ensuring users can refine their queries without losing track of previous answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DocChat Workflow\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/mbb8Z17CXEGPYd3V5FhZYg/project-overview.jpg\" alt=\"project overview\">\n",
    "\n",
    "1ï¸âƒ£ User query processing & relevance analysis\n",
    "- The system starts when a user submits a question about their uploaded document(s).\n",
    "- Before retrieving any data, DocChat first analyzes query relevance to determine if the question is within the scope of the uploaded content.\n",
    "\n",
    "2ï¸âƒ£ Routing & query categorization\n",
    "- The query is routed through an intelligent agent that decides whether the system can answer it using the document(s):\n",
    "    - âœ… In Scope: Proceed with document retrieval and response generation.\n",
    "    - âŒ Not in Scope: Inform the user that the question cannot be answered based on the provided documents, preventing hallucinations.\n",
    "\n",
    "3ï¸âƒ£ Multi-agent research & document retrieval\n",
    "- If the query is relevant, DocChat retrieves relevant document sections from a hybrid search system:\n",
    "    - Docling converts the document into a structured Markdown format for better chunking.\n",
    "    - LangChain splits the document into logical chunks based on headers and stores them in ChromaDB (a vector store).\n",
    "    - The retrieval module searches for the most contextually relevant document chunks using BM25 and vector search.\n",
    "\n",
    "4ï¸âƒ£ Answer generation & verification loop\n",
    "- Conduct research:\n",
    "    - The Research Agent generates an initial answer based on retrieved content.\n",
    "    - A sub-process starts where queries are dynamically generated for more precise retrieval.\n",
    "\n",
    "- Verification process:\n",
    "    - The Verification Agent cross-checks the generated response against the retrieved content.\n",
    "    - If the response is fully supported, the system finalizes and returns the answer.\n",
    "    - If verification fails (e.g., hallucinations, unsupported claims), the system re-runs the research step until a verifiable response is found.\n",
    "\n",
    "5ï¸âƒ£ Response finalization\n",
    "- After verification is complete, DocChat returns the final response to the user.\n",
    "- The workflow ensures that each answer is sourced directly from the provided document(s), preventing fabrication or unreliable outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Parsing with Docling\n",
    "\n",
    "Processing PDFs with complex structures, tables, and intricate layouts requires careful selection of a reliable document parsing tool. Many libraries struggle with accuracy when dealing with nested tables, multi-column formats, or scanned PDFs, often resulting in misaligned text, missing data, or broken layouts.\n",
    "\n",
    "To overcome these challenges, DocChat leverages [Docling](https://github.com/docling-project/docling)â€”an open-source document processing library designed for high-precision parsing and structured data extraction.\n",
    "\n",
    "### Why Docling?\n",
    "\n",
    "âœ… Accurate Table & Layout Parsing â€“ Recognizes complex table structures, reading sequences, and multi-column layouts.\n",
    "\n",
    "âœ… Multi-Format Support â€“ Reads and exports documents in Markdown, JSON, PDF, DOCX, PPTX, XLSX, HTML, AsciiDoc, and images.\n",
    "\n",
    "âœ… OCR for Scanned PDFs â€“ Extracts text from scanned documents using optical character recognition (OCR).\n",
    "\n",
    "âœ… Seamless Integration with LangChain â€“ Enables structured chunking for better retrieval and vector search in ChromaDB.\n",
    "\n",
    "### How Docling parses text?\n",
    "\n",
    "- Uses `DocumentConverter` to extract structured content.\n",
    "- Converts the PDF into Markdown format.\n",
    "- Splits the extracted content based on headers using `MarkdownHeaderTextSplitter`.\n",
    "- Prints the full extracted sections for review.\n",
    "\n",
    "Docling essentially has a built-in OCR (Optical Character Recognition) capabilities, so it can process PDFs that contain scanned images of text, making it effective for handling historical documents, academic papers, or other non-digitally generated content. Docling is equipped to handle both structured and unstructured PDFs, including scanned documents, making it a far more versatile and reliable tool for text extraction in complex scenarios. This demonstrates Doclingâ€™s advantage in working with real-world PDFs, where many documents are scanned rather than digitally created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Vector Database with ChromaDB\n",
    "\n",
    "Once documents have been parsed and structured using Docling, the next step is to efficiently store and retrieve relevant document chunks. This is where ChromaDB comes into playâ€”a high-performance vector database optimized for fast and accurate similarity search.\n",
    "\n",
    "### What is ChromaDB?\n",
    "[Chroma DB](https://github.com/chroma-core/chroma) is an open-source vector database optimized for fast and scalable similarity search. It enables efficient storage, retrieval, and ranking of document embeddings, making it a key component of RAG workflows.\n",
    "\n",
    "### Why ChromaDB?\n",
    "âœ… Blazing-Fast Vector Search â€“ Finds the most relevant document chunks in milliseconds.\n",
    "\n",
    "âœ… Persistent Storage â€“ Keeps embeddings saved for reuse across sessions.\n",
    "\n",
    "âœ… Seamless LangChain Integration â€“ Works natively with LangChain for retrieval-augmented generation (RAG).\n",
    "\n",
    "âœ… Scalable and Lightweight â€“ Handles millions of embeddings efficiently without complex infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Configuration\n",
    "\n",
    "We configure the logging system to provide informative output during execution. This helps in debugging and tracking the flow of our program, showing timestamps, module names, and message levels (INFO, ERROR, etc.).\n",
    "\n",
    "### Configuration Settings\n",
    "The Settings class defines configuration parameters for our system:\n",
    "\n",
    "- `CACHE_DIR`: Where processed document chunks are cached to avoid reprocessing\n",
    "- `CACHE_EXPIRE_DAYS`: How long to keep cached documents before refreshing\n",
    "- `CHROMA_DB_PATH`: Where the vector database stores embeddings\n",
    "- `VECTOR_SEARCH_K`: How many similar vectors to retrieve when searching\n",
    "- `HYBRID_RETRIEVER_WEIGHTS`: Balance between keyword-based (BM25) and semantic (vector) search\n",
    "\n",
    "### Constants\n",
    "These constants define limits and constraints:\n",
    "\n",
    "- `MAX_TOTAL_SIZE`: Maximum total size of documents (100MB)\n",
    "- `ALLOWED_TYPES`: File extensions that our system can process\n",
    "\n",
    "### IBM Watson AI Credentials\n",
    "This section initializes the connection to IBM Watson AI services:\n",
    "\n",
    "- Creates credentials for accessing IBM WatsonX AI\n",
    "- Initializes the API client with those credentials\n",
    "- This enables our system to use IBM's language models for document processing and question answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:59:44,345 - ibm_watsonx_ai.client - INFO - Client successfully initialized\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration settings\n",
    "class Settings:\n",
    "    CACHE_DIR = \"./cache\"\n",
    "    CACHE_EXPIRE_DAYS = 7\n",
    "    CHROMA_DB_PATH = \"./chroma_db\"\n",
    "    VECTOR_SEARCH_K = 5\n",
    "    HYBRID_RETRIEVER_WEIGHTS = [0.5, 0.5]\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "MAX_TOTAL_SIZE = 100 * 1024 * 1024  # 100MB\n",
    "ALLOWED_TYPES = ['.pdf', '.docx', '.txt', '.md']\n",
    "\n",
    "# IBM Watson credentials\n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "credentials = Credentials(url=\"https://us-south.ml.cloud.ibm.com\")\n",
    "client = APIClient(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessor Module\n",
    "\n",
    "The `DocumentProcessor` class is responsible for handling document parsing, caching, and chunking. It ensures efficient processing by:\n",
    "\n",
    "- **Validating file sizes** before processing.\n",
    "- **Using caching** to avoid redundant processing of previously uploaded files.\n",
    "- **Extracting structured content** from documents using Docling.\n",
    "- **Splitting text into chunks** using **MarkdownHeaderTextSplitter** for better retrieval in vector databases.\n",
    "\n",
    "### Function Breakdown\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `__init__()` | Initializes cache directory and header settings. |\n",
    "| `validate_files(files: List)` | Ensures uploaded files do not exceed the size limit. |\n",
    "| `process(files: List) -> List` | Handles document processing, caching, and deduplication. |\n",
    "| `_process_file(file) -> List` | Converts a document into Markdown and splits it into chunks. |\n",
    "| `_generate_hash(content: bytes) -> str` | Creates a unique hash of file content. |\n",
    "| `_save_to_cache(chunks: List, cache_path: Path)` | Saves processed document chunks to cache. |\n",
    "| `_load_from_cache(cache_path: Path) -> List` | Loads cached document chunks if available. |\n",
    "| `_is_cache_valid(cache_path: Path) -> bool` | Checks if a cached file is still valid and not corrupted. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Document Processing\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.headers = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\")]\n",
    "        self.cache_dir = Path(settings.CACHE_DIR)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def validate_files(self, files: List) -> None:\n",
    "        \"\"\"Validate the total size of the uploaded files.\"\"\"\n",
    "        total_size = sum(os.path.getsize(f.name) for f in files)\n",
    "        if total_size > MAX_TOTAL_SIZE:\n",
    "            raise ValueError(f\"Total size exceeds {MAX_TOTAL_SIZE//1024//1024}MB limit\")\n",
    "\n",
    "    def process(self, files: List) -> List:\n",
    "        \"\"\"Process files with caching for subsequent queries\"\"\"\n",
    "        self.validate_files(files)\n",
    "        all_chunks = []\n",
    "        seen_hashes = set()\n",
    "        \n",
    "        for file in files:\n",
    "            try:\n",
    "                # Generate content-based hash for caching\n",
    "                with open(file.name, \"rb\") as f:\n",
    "                    file_hash = self._generate_hash(f.read())\n",
    "                \n",
    "                cache_path = self.cache_dir / f\"{file_hash}.pkl\"\n",
    "                \n",
    "                if self._is_cache_valid(cache_path):\n",
    "                    logger.info(f\"Loading from cache: {file.name}\")\n",
    "                    chunks = self._load_from_cache(cache_path)\n",
    "                else:\n",
    "                    logger.info(f\"Processing and caching: {file.name}\")\n",
    "                    chunks = self._process_file(file)\n",
    "                    self._save_to_cache(chunks, cache_path)\n",
    "                \n",
    "                # Deduplicate chunks across files\n",
    "                for chunk in chunks:\n",
    "                    chunk_hash = self._generate_hash(chunk.page_content.encode())\n",
    "                    if chunk_hash not in seen_hashes:\n",
    "                        all_chunks.append(chunk)\n",
    "                        seen_hashes.add(chunk_hash)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {file.name}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        logger.info(f\"Total unique chunks: {len(all_chunks)}\")\n",
    "        return all_chunks\n",
    "\n",
    "    def _process_file(self, file) -> List:\n",
    "        \"\"\"Original processing logic with Docling\"\"\"\n",
    "        if not file.name.endswith(('.pdf', '.docx', '.txt', '.md')):\n",
    "            logger.warning(f\"Skipping unsupported file type: {file.name}\")\n",
    "            return []\n",
    "\n",
    "        converter = DocumentConverter()\n",
    "        markdown = converter.convert(file.name).document.export_to_markdown()\n",
    "        splitter = MarkdownHeaderTextSplitter(self.headers)\n",
    "        return splitter.split_text(markdown)\n",
    "\n",
    "    def _generate_hash(self, content: bytes) -> str:\n",
    "        return hashlib.sha256(content).hexdigest()\n",
    "\n",
    "    def _save_to_cache(self, chunks: List, cache_path: Path):\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"timestamp\": datetime.now().timestamp(),\n",
    "                \"chunks\": chunks\n",
    "            }, f)\n",
    "\n",
    "    def _load_from_cache(self, cache_path: Path) -> List:\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data[\"chunks\"]\n",
    "\n",
    "    def _is_cache_valid(self, cache_path: Path) -> bool:\n",
    "        if not cache_path.exists():\n",
    "            return False\n",
    "        \n",
    "        # Check if file size is too small (possibly corrupt)\n",
    "        if cache_path.stat().st_size < 100:  # Arbitrary small size\n",
    "            return False\n",
    "            \n",
    "        cache_age = datetime.now() - datetime.fromtimestamp(cache_path.stat().st_mtime)\n",
    "        return cache_age < timedelta(days=settings.CACHE_EXPIRE_DAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph multi-agent system structure\n",
    "\n",
    "A [multi-agent system (MAS)](https://www.ibm.com/think/topics/multiagent-system?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-nutricoach-v1_1737477753) is composed of multiple artificial intelligence (AI) agents collaborating to carry out tasks for a user or another system. Here are some of the difference between Agentic AI and MAS.\n",
    "\n",
    "| Characteristic | Agentic AI | Multi-Agent Systems (MAS) |\n",
    "|----------------|------------|---------------------------|\n",
    "| **Autonomy** | Central focusâ€”autonomous task execution | May include agents with varying levels of autonomy |\n",
    "| **Interaction** | Limited to tools, systems, or environments | Key focusâ€”agents interact, communicate, and coordinate |\n",
    "| **Scope** | Individual agent | Multiple agents in a shared system |\n",
    "| **Dependency** | Agentic AI can exist independently | MAS may involve agentic AI but doesn't require it |\n",
    "\n",
    "Here, we are going to use [LangGraph](https://langchain-ai.github.io/langgraph/) is an open-source Python framework designed for multi-agent workflows in AI applications. It extends LangChain by enabling graph-based state management, making it easier to coordinate multiple AI agents in structured workflows. LangGraph is particularly useful in RAG and multi-step reasoning, where multiple agents collaborate to refine, verify, and improve responses dynamically.\n",
    "\n",
    "### How Does LangGraph Work?\n",
    "\n",
    "LangGraph operates on the principle of stateful workflows, where each step in the process is defined as a node in a directed graph. The edges define transitions between nodes based on logic.\n",
    "\n",
    "A LangGraph workflow consists of:\n",
    "\n",
    "- **Nodes â†’** Represent individual processing steps (e.g., research, verification).\n",
    "- **Edges â†’** Define the flow of execution (e.g., go to verification after research).\n",
    "- **State Objects â†’** Store data passed between agents.\n",
    "- **Conditional Transitions â†’** Allow decision-making between nodes.\n",
    "\n",
    "### Graph Structure for this Project\n",
    "The AgentWorkflow class constructs the multi-agent system using LangGraphâ€™s `StateGraph`, ensuring a structured approach to information retrieval and verification.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/3u1WREmhVmC-X-BzeFWIZw/graph-diagram.jpg\" width=80% height=80% alt=\"graph-diagram\"><br>\n",
    "\n",
    "#### Workflow Breakdown\n",
    "1ï¸âƒ£ **Check Relevance** â€“ The `RelevanceChecker` determines if the query can be answered based on the retrieved documents.\n",
    "- If relevant â†’ Proceed to research.\n",
    "- If irrelevant â†’ Terminate workflow.\n",
    "\n",
    "2ï¸âƒ£ **Research Step** â€“ The `ResearchAgent` generates a draft answer using relevant documents.\n",
    "\n",
    "3ï¸âƒ£ **Verification Step** â€“ The `VerificationAgent` assesses the draft answer for accuracy and relevance.\n",
    "\n",
    "4ï¸âƒ£ **Decision Making** â€“ Based on verification:\n",
    "- If the answer lacks support â†’ Re-research and refine.\n",
    "- If verified â†’ End workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Checker: ensuring query-document alignment\n",
    "\n",
    "The `RelevanceChecker` is responsible for determining whether retrieved documents contain relevant information to answer a given question. It uses an **ensemble retriever** to fetch document chunks and then leverages IBM WatsonX AI for classification. The goal is to categorize relevance into three possible labels:\n",
    "\n",
    "* **\"CAN_ANSWER\"** â€“ The documents provide sufficient information for a full answer.\n",
    "* **\"PARTIAL\"** â€“ The documents mention the topic but lack complete details.\n",
    "* **\"NO_MATCH\"** â€“ The documents do not discuss the question at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RelevanceChecker` follows a specific workflow to determine document relevance:\n",
    "\n",
    "1. **Document Retrieval**: When a question is received, the checker uses an ensemble retriever (combining BM25 and vector search) to fetch the top-k most relevant document chunks.\n",
    "\n",
    "2. **Content Aggregation**: The retrieved chunks are combined into a single text passage, preserving their individual content but merging them for analysis.\n",
    "\n",
    "3. **Prompt Engineering**: A carefully crafted prompt is sent to the IBM WatsonX Granite model, asking it to classify the relevance of the document content to the question. The prompt includes:\n",
    "  - Clear instructions about the classification task\n",
    "  - Detailed definitions of the three possible labels\n",
    "  - Guidelines for choosing between ambiguous cases\n",
    "  - Both the original question and the document content\n",
    "\n",
    "4. **LLM Classification**: The Granite-3-8b-instruct model processes the prompt and returns a single label classification.\n",
    "\n",
    "5. **Response Validation**: The checker validates that the response is one of the expected labels, defaulting to \"NO_MATCH\" if an invalid response is received.\n",
    "\n",
    "6. **Error Handling**: Comprehensive error handling captures API failures, unexpected response structures, or empty document returns, defaulting to \"NO_MATCH\" in problematic cases.\n",
    "\n",
    "7. **Workflow Integration**: The returned classification determines if the agent workflow should proceed to the research phase or respond with a \"cannot answer\" message to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance Checker\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "class RelevanceChecker:\n",
    "    def __init__(self):\n",
    "        # Initialize the WatsonX ModelInference\n",
    "        self.model = ModelInference(\n",
    "            model_id=\"ibm/granite-3-8b-instruct\",\n",
    "            credentials=credentials,\n",
    "            project_id=\"skills-network\",\n",
    "            params={\"temperature\": 0, \"max_tokens\": 10},\n",
    "        )\n",
    "\n",
    "    def check(self, question: str, retriever, k=3) -> str:\n",
    "        \"\"\"\n",
    "        1. Retrieve the top-k document chunks from the global retriever.\n",
    "        2. Combine them into a single text string.\n",
    "        3. Pass that text + question to the LLM for classification.\n",
    "\n",
    "        Returns: \"CAN_ANSWER\", \"PARTIAL\", or \"NO_MATCH\".\n",
    "        \"\"\"\n",
    "        logger.debug(f\"RelevanceChecker.check called with question='{question}' and k={k}\")\n",
    "\n",
    "        # Retrieve doc chunks from the ensemble retriever\n",
    "        top_docs = retriever.invoke(question)\n",
    "        if not top_docs:\n",
    "            logger.debug(\"No documents returned from retriever.invoke(). Classifying as NO_MATCH.\")\n",
    "            return \"NO_MATCH\"\n",
    "\n",
    "        # Combine the top k chunk texts into one string\n",
    "        document_content = \"\\n\\n\".join(doc.page_content for doc in top_docs[:k])\n",
    "\n",
    "        # Create a prompt for the LLM to classify relevance\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI relevance checker between a user's question and provided document content.\n",
    "\n",
    "        **Instructions:**\n",
    "        - Classify how well the document content addresses the user's question.\n",
    "        - Respond with only one of the following labels: CAN_ANSWER, PARTIAL, NO_MATCH.\n",
    "        - Do not include any additional text or explanation.\n",
    "\n",
    "        **Labels:**\n",
    "        1) \"CAN_ANSWER\": The passages contain enough explicit information to fully answer the question.\n",
    "        2) \"PARTIAL\": The passages mention or discuss the question's topic but do not provide all the details needed for a complete answer.\n",
    "        3) \"NO_MATCH\": The passages do not discuss or mention the question's topic at all.\n",
    "\n",
    "        **Important:** If the passages mention or reference the topic or timeframe of the question in any way, even if incomplete, respond with \"PARTIAL\" instead of \"NO_MATCH\".\n",
    "\n",
    "        **Question:** {question}\n",
    "        **Passages:** {document_content}\n",
    "\n",
    "        **Respond ONLY with one of the following labels: CAN_ANSWER, PARTIAL, NO_MATCH**\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the LLM\n",
    "        try:\n",
    "            response = self.model.chat(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt  # Changed from list to string\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model inference: {e}\")\n",
    "            return \"NO_MATCH\"\n",
    "\n",
    "        # Extract the content from the response\n",
    "        try:\n",
    "            llm_response = response['choices'][0]['message']['content'].strip().upper()\n",
    "            logger.debug(f\"LLM response: {llm_response}\")\n",
    "        except (IndexError, KeyError) as e:\n",
    "            logger.error(f\"Unexpected response structure: {e}\")\n",
    "            return \"NO_MATCH\"\n",
    "\n",
    "        print(f\"Checker response: {llm_response}\")\n",
    "\n",
    "        # Validate the response\n",
    "        valid_labels = {\"CAN_ANSWER\", \"PARTIAL\", \"NO_MATCH\"}\n",
    "        if llm_response not in valid_labels:\n",
    "            logger.debug(\"LLM did not respond with a valid label. Forcing 'NO_MATCH'.\")\n",
    "            classification = \"NO_MATCH\"\n",
    "        else:\n",
    "            logger.debug(f\"Classification recognized as '{llm_response}'.\")\n",
    "            classification = llm_response\n",
    "\n",
    "        return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Agent: Generating Document-Based Answers\n",
    "\n",
    "The `ResearchAgent` is responsible for **generating an initial draft answer** using retrieved documents. It interacts with **IBM WatsonX AI** to synthesize responses based on relevant content. This step is crucial in the RAG pipeline, ensuring that AI-generated answers are grounded in the provided data.\n",
    "\n",
    "### Key Functions of the Research Agent\n",
    "âœ… **Context-Aware Answer Generation** â€“ Produces fact-based responses using retrieved documents.\n",
    "\n",
    "âœ… **Structured Prompting** â€“ Ensures the AI model adheres to precise instructions for accurate outputs.\n",
    "\n",
    "âœ… **Response Sanitization** â€“ Cleans and formats LLM responses for better readability.\n",
    "\n",
    "### How the Research Agent Works\n",
    "\n",
    "1. **Document Aggregation**: When provided with relevant document chunks, the agent combines them into a single context string.\n",
    "\n",
    "2. **Prompt Construction**: A well-structured prompt is created that includes:\n",
    "  - Clear instructions about answering based only on provided context\n",
    "  - Guidelines for clarity, conciseness, and factuality\n",
    "  - The user's original question\n",
    "  - The aggregated document context\n",
    "\n",
    "3. **Model Invocation**: The prompt is sent to the Meta Llama 3 90B model (via IBM WatsonX), configured with:\n",
    "  - A moderate temperature (0.3) for balance between creativity and determinism\n",
    "  - Appropriate token limits (300) for concise but informative answers\n",
    "\n",
    "4. **Response Processing**: The model's response is extracted from the API return value and sanitized to remove unnecessary whitespace.\n",
    "\n",
    "5. **Error Handling**: Comprehensive error handling addresses API failures and unexpected response structures, providing fallback responses when necessary.\n",
    "\n",
    "6. **Result Packaging**: The final answer is returned along with the context used, enabling verification in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Agent\n",
    "class ResearchAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the research agent with the IBM WatsonX ModelInference.\n",
    "        \"\"\"\n",
    "        # Initialize the WatsonX ModelInference\n",
    "        print(\"Initializing ResearchAgent with IBM WatsonX ModelInference...\")\n",
    "        self.model = ModelInference(\n",
    "            model_id=\"meta-llama/llama-3-2-90b-vision-instruct\", \n",
    "            credentials=credentials,\n",
    "            project_id=\"skills-network\",\n",
    "            params={\n",
    "                \"max_tokens\": 300,            # Adjust based on desired response length\n",
    "                \"temperature\": 0.3,           # Controls randomness; lower values make output more deterministic\n",
    "            }\n",
    "        )\n",
    "        print(\"ModelInference initialized successfully.\")\n",
    "\n",
    "    def sanitize_response(self, response_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize the LLM's response by stripping unnecessary whitespace.\n",
    "        \"\"\"\n",
    "        return response_text.strip()\n",
    "\n",
    "    def generate_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a structured prompt for the LLM to generate a precise and factual answer.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant designed to provide precise and factual answers based on the given context.\n",
    "\n",
    "        **Instructions:**\n",
    "        - Answer the following question using only the provided context.\n",
    "        - Be clear, concise, and factual.\n",
    "        - Return as much information as you can get from the context.\n",
    "        \n",
    "        **Question:** {question}\n",
    "        **Context:**\n",
    "        {context}\n",
    "\n",
    "        **Provide your answer below:**\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def generate(self, question: str, documents: List[Document]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate an initial answer using the provided documents.\n",
    "        \"\"\"\n",
    "        print(f\"ResearchAgent.generate called with question='{question}' and {len(documents)} documents.\")\n",
    "\n",
    "        # Combine the top document contents into one string\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        print(f\"Combined context length: {len(context)} characters.\")\n",
    "\n",
    "        # Create a prompt for the LLM\n",
    "        prompt = self.generate_prompt(question, context)\n",
    "        print(\"Prompt created for the LLM.\")\n",
    "\n",
    "        # Call the LLM to generate the answer\n",
    "        try:\n",
    "            print(\"Sending prompt to the model...\")\n",
    "            response = self.model.chat(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt  # Ensure content is a string\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            print(\"LLM response received.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model inference: {e}\")\n",
    "            raise RuntimeError(\"Failed to generate answer due to a model error.\") from e\n",
    "\n",
    "        # Extract and process the LLM's response\n",
    "        try:\n",
    "            llm_response = response['choices'][0]['message']['content'].strip()\n",
    "            print(f\"Raw LLM response:\\n{llm_response}\")\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print(f\"Unexpected response structure: {e}\")\n",
    "            llm_response = \"I cannot answer this question based on the provided documents.\"\n",
    "\n",
    "        # Sanitize the response\n",
    "        draft_answer = self.sanitize_response(llm_response) if llm_response else \"I cannot answer this question based on the provided documents.\"\n",
    "\n",
    "        print(f\"Generated answer: {draft_answer}\")\n",
    "\n",
    "        return {\n",
    "            \"draft_answer\": draft_answer,\n",
    "            \"context_used\": context\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Agent: Validating Answer Accuracy and Relevance\n",
    "\n",
    "The `VerificationAgent` is responsible for **fact-checking and validating generated answers** using the retrieved documents. This agent ensures that the AI-generated response is:\n",
    "1. **Supported by factual evidence** from the documents.\n",
    "2. **Free from contradictions** or misinformation.\n",
    "3. **Relevant to the original question.**\n",
    "It interacts with **IBM WatsonX AI** to analyze the relationship between the answer and its source documents, producing a structured verification report.\n",
    "\n",
    "### How the Verification Agent Works\n",
    "\n",
    "1. **Evidence Aggregation**: When provided with an answer and relevant document chunks, the agent combines all document contents into a comprehensive context string.\n",
    "\n",
    "2. **Structured Verification Prompt**: A detailed prompt is created that instructs the model to:\n",
    "  - Compare the answer against the provided context\n",
    "  - Check for factual support (direct or indirect)\n",
    "  - Identify any unsupported claims\n",
    "  - Detect contradictions between the answer and context\n",
    "  - Assess relevance to the question\n",
    "  - Provide any additional explanatory details\n",
    "\n",
    "3. **Zero-Temperature Inference**: The prompt is sent to IBM's Granite 3-8b-instruct model with temperature set to 0.0 for maximum consistency and determinism in verification.\n",
    "\n",
    "4. **Response Parsing**: The model's response is parsed into a structured format with specific fields:\n",
    "  - Supported: YES/NO\n",
    "  - Unsupported Claims: List of claims\n",
    "  - Contradictions: List of contradictions\n",
    "  - Relevant: YES/NO\n",
    "  - Additional Details: Explanatory text\n",
    "\n",
    "5. **Formatting for Readability**: The structured verification data is formatted into a human-readable report with clear section headers and concise content.\n",
    "\n",
    "6. **Error Handling**: Comprehensive error handling addresses issues like:\n",
    "  - API failures\n",
    "  - Unexpected response structures\n",
    "  - Empty responses\n",
    "  - Failures in response parsing\n",
    "  - Each providing appropriate fallback verification reports\n",
    "\n",
    "7. **Workflow Integration**: The verification report determines whether the answer is trustworthy or if the workflow should cycle back for additional research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification Agent\n",
    "class VerificationAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the verification agent with the IBM WatsonX ModelInference.\n",
    "        \"\"\"\n",
    "        # Initialize the WatsonX ModelInference\n",
    "        print(\"Initializing VerificationAgent with IBM WatsonX ModelInference...\")\n",
    "        self.model = ModelInference(\n",
    "            model_id=\"ibm/granite-3-8b-instruct\", \n",
    "            credentials=credentials,\n",
    "            project_id=\"skills-network\",\n",
    "            params={\n",
    "                \"max_tokens\": 200,            # Adjust based on desired response length\n",
    "                \"temperature\": 0.0,           # Remove randomness for consistency\n",
    "            }\n",
    "        )\n",
    "        print(\"ModelInference initialized successfully.\")\n",
    "\n",
    "    def sanitize_response(self, response_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize the LLM's response by stripping unnecessary whitespace.\n",
    "        \"\"\"\n",
    "        return response_text.strip()\n",
    "\n",
    "    def generate_prompt(self, answer: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a structured prompt for the LLM to verify the answer against the context.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant designed to verify the accuracy and relevance of answers based on provided context.\n",
    "\n",
    "        **Instructions:**\n",
    "        - Verify the following answer against the provided context.\n",
    "        - Check for:\n",
    "        1. Direct/indirect factual support (YES/NO)\n",
    "        2. Unsupported claims (list any if present)\n",
    "        3. Contradictions (list any if present)\n",
    "        4. Relevance to the question (YES/NO)\n",
    "        - Provide additional details or explanations where relevant.\n",
    "        - Respond in the exact format specified below without adding any unrelated information.\n",
    "\n",
    "        **Format:**\n",
    "        Supported: YES/NO\n",
    "        Unsupported Claims: [item1, item2, ...]\n",
    "        Contradictions: [item1, item2, ...]\n",
    "        Relevant: YES/NO\n",
    "        Additional Details: [Any extra information or explanations]\n",
    "\n",
    "        **Answer:** {answer}\n",
    "        **Context:**\n",
    "        {context}\n",
    "\n",
    "        **Respond ONLY with the above format.**\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def parse_verification_response(self, response_text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse the LLM's verification response into a structured dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lines = response_text.split('\\n')\n",
    "            verification = {}\n",
    "            for line in lines:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    key = key.strip().capitalize()\n",
    "                    value = value.strip()\n",
    "                    if key in {\"Supported\", \"Unsupported claims\", \"Contradictions\", \"Relevant\", \"Additional details\"}:\n",
    "                        if key in {\"Unsupported claims\", \"Contradictions\"}:\n",
    "                            # Convert string list to actual list\n",
    "                            if value.startswith('[') and value.endswith(']'):\n",
    "                                items = value[1:-1].split(',')\n",
    "                                # Remove any surrounding quotes and whitespace\n",
    "                                items = [item.strip().strip('\"').strip(\"'\") for item in items if item.strip()]\n",
    "                                verification[key] = items\n",
    "                            else:\n",
    "                                verification[key] = []\n",
    "                        elif key == \"Additional details\":\n",
    "                            verification[key] = value\n",
    "                        else:\n",
    "                            verification[key] = value.upper()\n",
    "            # Ensure all keys are present\n",
    "            for key in [\"Supported\", \"Unsupported Claims\", \"Contradictions\", \"Relevant\", \"Additional Details\"]:\n",
    "                if key not in verification:\n",
    "                    if key in {\"Unsupported Claims\", \"Contradictions\"}:\n",
    "                        verification[key] = []\n",
    "                    elif key == \"Additional Details\":\n",
    "                        verification[key] = \"\"\n",
    "                    else:\n",
    "                        verification[key] = \"NO\"\n",
    "\n",
    "            return verification\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing verification response: {e}\")\n",
    "            return None\n",
    "\n",
    "    def format_verification_report(self, verification: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Format the verification report dictionary into a readable paragraph.\n",
    "        \"\"\"\n",
    "        supported = verification.get(\"Supported\", \"NO\")\n",
    "        unsupported_claims = verification.get(\"Unsupported Claims\", [])\n",
    "        contradictions = verification.get(\"Contradictions\", [])\n",
    "        relevant = verification.get(\"Relevant\", \"NO\")\n",
    "        additional_details = verification.get(\"Additional Details\", \"\")\n",
    "\n",
    "        report = f\"**Supported:** {supported}\\n\"\n",
    "        if unsupported_claims:\n",
    "            report += f\"**Unsupported Claims:** {', '.join(unsupported_claims)}\\n\"\n",
    "        else:\n",
    "            report += f\"**Unsupported Claims:** None\\n\"\n",
    "\n",
    "        if contradictions:\n",
    "            report += f\"**Contradictions:** {', '.join(contradictions)}\\n\"\n",
    "        else:\n",
    "            report += f\"**Contradictions:** None\\n\"\n",
    "\n",
    "        report += f\"**Relevant:** {relevant}\\n\"\n",
    "\n",
    "        if additional_details:\n",
    "            report += f\"**Additional Details:** {additional_details}\\n\"\n",
    "        else:\n",
    "            report += f\"**Additional Details:** None\\n\"\n",
    "\n",
    "        return report\n",
    "\n",
    "    def check(self, answer: str, documents: List[Document]) -> Dict:\n",
    "        \"\"\"\n",
    "        Verify the answer against the provided documents.\n",
    "        \"\"\"\n",
    "        print(f\"VerificationAgent.check called with answer='{answer}' and {len(documents)} documents.\")\n",
    "\n",
    "        # Combine all document contents into one string without truncation\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        print(f\"Combined context length: {len(context)} characters.\")\n",
    "\n",
    "        # Create a prompt for the LLM to verify the answer\n",
    "        prompt = self.generate_prompt(answer, context)\n",
    "        print(\"Prompt created for the LLM.\")\n",
    "\n",
    "        # Call the LLM to generate the verification report\n",
    "        try:\n",
    "            print(\"Sending prompt to the model...\")\n",
    "            response = self.model.chat(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt  # Ensure content is a string\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            print(\"LLM response received.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model inference: {e}\")\n",
    "            raise RuntimeError(\"Failed to verify answer due to a model error.\") from e\n",
    "\n",
    "        # Extract and process the LLM's response\n",
    "        try:\n",
    "            llm_response = response['choices'][0]['message']['content'].strip()\n",
    "            print(f\"Raw LLM response:\\n{llm_response}\")\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print(f\"Unexpected response structure: {e}\")\n",
    "            verification_report = {\n",
    "                \"Supported\": \"NO\",\n",
    "                \"Unsupported Claims\": [],\n",
    "                \"Contradictions\": [],\n",
    "                \"Relevant\": \"NO\",\n",
    "                \"Additional Details\": \"Invalid response structure from the model.\"\n",
    "            }\n",
    "            verification_report_formatted = self.format_verification_report(verification_report)\n",
    "            print(f\"Verification report:\\n{verification_report_formatted}\")\n",
    "            print(f\"Context used: {context}\")\n",
    "            return {\n",
    "                \"verification_report\": verification_report_formatted,\n",
    "                \"context_used\": context\n",
    "            }\n",
    "\n",
    "        # Sanitize the response\n",
    "        sanitized_response = self.sanitize_response(llm_response) if llm_response else \"\"\n",
    "        if not sanitized_response:\n",
    "            print(\"LLM returned an empty response.\")\n",
    "            verification_report = {\n",
    "                \"Supported\": \"NO\",\n",
    "                \"Unsupported Claims\": [],\n",
    "                \"Contradictions\": [],\n",
    "                \"Relevant\": \"NO\",\n",
    "                \"Additional Details\": \"Empty response from the model.\"\n",
    "            }\n",
    "        else:\n",
    "            # Parse the response into the expected format\n",
    "            verification_report = self.parse_verification_response(sanitized_response)\n",
    "            if verification_report is None:\n",
    "                print(\"LLM did not respond with the expected format. Using default verification report.\")\n",
    "                verification_report = {\n",
    "                    \"Supported\": \"NO\",\n",
    "                    \"Unsupported Claims\": [],\n",
    "                    \"Contradictions\": [],\n",
    "                    \"Relevant\": \"NO\",\n",
    "                    \"Additional Details\": \"Failed to parse the model's response.\"\n",
    "                }\n",
    "\n",
    "        # Format the verification report into a paragraph\n",
    "        verification_report_formatted = self.format_verification_report(verification_report)\n",
    "        print(f\"Verification report:\\n{verification_report_formatted}\")\n",
    "        print(f\"Context used: {context}\")\n",
    "\n",
    "        return {\n",
    "            \"verification_report\": verification_report_formatted,\n",
    "            \"context_used\": context\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid retriever: combining BM25 and vector search for optimal document retrieval\n",
    "\n",
    "The `RetrieverBuilder` class implements a **hybrid retrieval system** by combining:\n",
    "1. **BM25 (Lexical Search)** â€“ Traditional keyword-based retrieval.\n",
    "2. **Vector Search (Embedding-based)** â€“ Semantic retrieval using embeddings.\n",
    "\n",
    "This combination enhances the accuracy of RAG by leveraging the strengths of both approaches.\n",
    "\n",
    "### Why use a hybrid retriever?\n",
    "âœ… **Improves Recall** â€“ Captures both exact keyword matches and semantically similar content.\n",
    "\n",
    "âœ… **Balances Precision & Relevance** â€“ BM25 retrieves highly precise keyword matches, while vector retrieval finds related concepts.\n",
    "\n",
    "âœ… **Handles Misspellings & Variations** â€“ Vector embeddings allow for **fuzzy matching** beyond exact keyword searches.\n",
    "\n",
    "âœ… **Optimized for Multi-Agent Systems** â€“ Ensures robust document retrieval before passing data to AI agents.\n",
    "\n",
    "### Why this is essential for RAG?\n",
    "âœ… Ensures high-quality document retrieval for multi-agent research workflows.\n",
    "\n",
    "âœ… Improves AI response accuracy by providing both keyword-based and semantic matches.\n",
    "\n",
    "âœ… Enhances retrieval diversity, ensuring no relevant document is overlooked.\n",
    "\n",
    "With hybrid retrieval, the system achieves a balance between precision and recall, ensuring AI-generated responses are grounded in the most relevant information.\n",
    "\n",
    "### How the Hybrid Retriever Works\n",
    "\n",
    "1. **Embedding Initialization**: The builder initializes IBM WatsonX embeddings using the Slate-125m English retriever model, configured with specific parameters for token truncation and input text return options.\n",
    "\n",
    "2. **BM25 Creation**: When provided with document chunks, the system first creates a BM25 retriever, which uses a probabilistic retrieval model based on term frequency and inverse document frequency.\n",
    "\n",
    "3. **Vector Store Creation**: In parallel, it creates a Chroma vector store with a unique collection name based on the current timestamp, using the initialized WatsonX embeddings to convert document content into vector representations.\n",
    "\n",
    "4. **Vector Retriever Setup**: A retriever is created from the vector store, configured to return the top-k most similar documents (controlled by the VECTOR_SEARCH_K setting).\n",
    "\n",
    "5. **Ensemble Integration**: The BM25 and vector retrievers are combined into an EnsembleRetriever, with customizable weights (controlled by HYBRID_RETRIEVER_WEIGHTS) to balance their contributions.\n",
    "\n",
    "6. **Weighted Retrieval**: When a query is submitted, both retrievers independently find matching documents, and their results are combined based on the configured weights to produce a final, optimized result set.\n",
    "\n",
    "7. **Error Management**: Comprehensive error handling ensures that any failures in the retrieval pipeline are properly logged and reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever Builder\n",
    "class RetrieverBuilder:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the retriever builder with embeddings.\"\"\"\n",
    "        embed_params = {\n",
    "            EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "            EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "        }\n",
    "\n",
    "        watsonx_embedding = WatsonxEmbeddings(\n",
    "            model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "            project_id=\"skills-network\",\n",
    "            params=embed_params\n",
    "        )\n",
    "        self.embeddings = watsonx_embedding\n",
    "        \n",
    "    def build_hybrid_retriever(self, docs):\n",
    "        \"\"\"Build a hybrid retriever using BM25 and vector-based retrieval.\"\"\"\n",
    "        try:\n",
    "            # Create BM25 retriever\n",
    "            bm25 = BM25Retriever.from_documents(docs)\n",
    "            logger.info(\"BM25 retriever created successfully.\")\n",
    "            \n",
    "            # Create Chroma vector store with minimal parameters\n",
    "            import time\n",
    "            collection_name = f\"collection_{int(time.time())}\"\n",
    "            \n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=docs,\n",
    "                embedding=self.embeddings,\n",
    "                collection_name=collection_name  # Just use a unique collection name\n",
    "            )\n",
    "            logger.info(\"Vector store created successfully.\")\n",
    "            \n",
    "            # Create vector-based retriever\n",
    "            vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": settings.VECTOR_SEARCH_K})\n",
    "            logger.info(\"Vector retriever created successfully.\")\n",
    "            \n",
    "            # Combine retrievers into a hybrid retriever\n",
    "            hybrid_retriever = EnsembleRetriever(\n",
    "                retrievers=[bm25, vector_retriever],\n",
    "                weights=settings.HYBRID_RETRIEVER_WEIGHTS\n",
    "            )\n",
    "            logger.info(\"Hybrid retriever created successfully.\")\n",
    "            return hybrid_retriever\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to build hybrid retriever: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Workflow: Orchestrating the Multi-Agent RAG System\n",
    "\n",
    "The `AgentWorkflow` class serves as the **central orchestrator** of the multi-agent RAG system, leveraging LangGraph for state management and flow control. It coordinates the interactions between specialized agents to process questions, retrieve relevant documents, generate answers, and verify their accuracy.\n",
    "\n",
    "### Key Components of the Agent Workflow\n",
    "\n",
    "âœ… **State-Based Graph Architecture** â€“ Uses StateGraph to maintain and transition between different states of processing.\n",
    "\n",
    "âœ… **Conditional Branching Logic** â€“ Dynamically determines the next processing step based on agent outputs.\n",
    "\n",
    "âœ… **Feedback Loop Mechanism** â€“ Enables answer refinement through verification-driven research iterations.\n",
    "\n",
    "âœ… **Error Handling and Recovery** â€“ Provides robust error handling throughout the workflow.\n",
    "\n",
    "### How the Agent Workflow Works\n",
    "\n",
    "1. **Graph Construction**: During initialization, the workflow builds a directed graph with nodes representing different processing steps and edges defining the transitions between them.\n",
    "\n",
    "2. **Relevance Check**: The workflow begins with the relevance checker determining if the documents can answer the question:\n",
    "  - If relevant (CAN_ANSWER or PARTIAL), proceeds to research\n",
    "  - If irrelevant (NO_MATCH), provides a \"cannot answer\" response\n",
    "\n",
    "3. **Research Phase**: If documents are relevant, the research agent generates a draft answer using the retrieved document chunks.\n",
    "\n",
    "4. **Verification Phase**: The verification agent fact-checks the draft answer against the source documents.\n",
    "\n",
    "5. **Decision Point**: Based on the verification report:\n",
    "  - If issues are found (unsupported claims or irrelevance), the workflow cycles back to research\n",
    "  - If no issues, the workflow proceeds to completion\n",
    "\n",
    "6. **Final State**: The end step prepares the final output containing the verified answer and verification report.\n",
    "\n",
    "7. **Complete Pipeline**: The `full_pipeline` method ties everything together, initializing the state with the question and retriever, invoking the workflow, and returning the results.\n",
    "\n",
    "### Function Breakdown\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `__init__()` | Initializes the agents and compiles the workflow graph. |\n",
    "| `build_workflow()` | Constructs the StateGraph with nodes and conditional edges. |\n",
    "| `_check_relevance_step()` | Determines if documents are relevant to the question. |\n",
    "| `_decide_after_relevance_check()` | Routes to research or end based on relevance. |\n",
    "| `_research_step()` | Generates a draft answer using the research agent. |\n",
    "| `_verification_step()` | Verifies the accuracy of the draft answer. |\n",
    "| `_end_step()` | Prepares the final output. |\n",
    "| `_decide_next_step()` | Determines whether to refine the answer or complete. |\n",
    "| `full_pipeline()` | Executes the complete workflow from question to answer. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    draft_answer: str\n",
    "    verification_report: str\n",
    "    is_relevant: bool\n",
    "    retriever: EnsembleRetriever\n",
    "\n",
    "class AgentWorkflow:\n",
    "    def __init__(self):\n",
    "        self.researcher = ResearchAgent()\n",
    "        self.verifier = VerificationAgent()\n",
    "        self.relevance_checker = RelevanceChecker()\n",
    "        self.compiled_workflow = self.build_workflow()  # Compile once during initialization\n",
    "        \n",
    "    def build_workflow(self):\n",
    "        \"\"\"Create and compile the multi-agent workflow.\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"check_relevance\", self._check_relevance_step)\n",
    "        workflow.add_node(\"research\", self._research_step)\n",
    "        workflow.add_node(\"verify\", self._verification_step)\n",
    "        workflow.add_node(\"end\", self._end_step)\n",
    "        \n",
    "        # Define edges\n",
    "        workflow.set_entry_point(\"check_relevance\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"check_relevance\",\n",
    "            self._decide_after_relevance_check,\n",
    "            {\n",
    "                \"relevant\": \"research\",\n",
    "                \"irrelevant\": \"end\"\n",
    "            }\n",
    "        )\n",
    "        workflow.add_edge(\"research\", \"verify\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"verify\",\n",
    "            self._decide_next_step,\n",
    "            {\n",
    "                \"re_research\": \"research\",\n",
    "                \"end\": \"end\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # End node leads to completion\n",
    "        workflow.add_edge(\"end\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def _check_relevance_step(self, state: AgentState) -> Dict:\n",
    "        retriever = state[\"retriever\"]\n",
    "        classification = self.relevance_checker.check(\n",
    "            question=state[\"question\"], \n",
    "            retriever=retriever, \n",
    "            k=20\n",
    "        )\n",
    "\n",
    "        if classification == \"CAN_ANSWER\":\n",
    "            # We have enough info to proceed\n",
    "            return {\"is_relevant\": True}\n",
    "        elif classification == \"PARTIAL\":\n",
    "            # There's partial coverage, but we can still proceed\n",
    "            return {\"is_relevant\": True}\n",
    "        else:  # classification == \"NO_MATCH\"\n",
    "            return {\n",
    "                \"is_relevant\": False,\n",
    "                \"draft_answer\": \"This question isn't related (or there's no data) for your query. Please ask another question relevant to the uploaded document(s).\"\n",
    "            }\n",
    "\n",
    "    def _decide_after_relevance_check(self, state: AgentState) -> str:\n",
    "        decision = \"relevant\" if state[\"is_relevant\"] else \"irrelevant\"\n",
    "        print(f\"[DEBUG] _decide_after_relevance_check -> {decision}\")\n",
    "        return decision\n",
    "    \n",
    "    def _research_step(self, state: AgentState) -> Dict:\n",
    "        print(f\"[DEBUG] Entered _research_step with question='{state['question']}'\")\n",
    "        result = self.researcher.generate(state[\"question\"], state[\"documents\"])\n",
    "        print(\"[DEBUG] Researcher returned draft answer.\")\n",
    "        return {\"draft_answer\": result[\"draft_answer\"]}\n",
    "    \n",
    "    def _verification_step(self, state: AgentState) -> Dict:\n",
    "        print(\"[DEBUG] Entered _verification_step. Verifying the draft answer...\")\n",
    "        result = self.verifier.check(state[\"draft_answer\"], state[\"documents\"])\n",
    "        print(\"[DEBUG] VerificationAgent returned a verification report.\")\n",
    "        return {\"verification_report\": result[\"verification_report\"]}\n",
    "        \n",
    "    def _end_step(self, state: AgentState) -> Dict:\n",
    "        \"\"\"Final step that prepares the output\"\"\"\n",
    "        logger.info(\"Workflow reached end step\")\n",
    "        # This is just a passthrough step to match the flowchart\n",
    "        return {}\n",
    "    \n",
    "    def _decide_next_step(self, state: AgentState) -> str:\n",
    "        verification_report = state[\"verification_report\"]\n",
    "        print(f\"[DEBUG] _decide_next_step with verification_report='{verification_report}'\")\n",
    "        if \"Supported: NO\" in verification_report or \"Relevant: NO\" in verification_report:\n",
    "            logger.info(\"[DEBUG] Verification indicates re-research needed.\")\n",
    "            return \"re_research\"\n",
    "        else:\n",
    "            logger.info(\"[DEBUG] Verification successful, ending workflow.\")\n",
    "            return \"end\"\n",
    "    \n",
    "    def full_pipeline(self, question: str, retriever: EnsembleRetriever):\n",
    "        try:\n",
    "            print(f\"[DEBUG] Starting full_pipeline with question='{question}'\")\n",
    "            documents = retriever.invoke(question)\n",
    "            logger.info(f\"Retrieved {len(documents)} relevant documents (from .invoke)\")\n",
    "\n",
    "            initial_state = AgentState(\n",
    "                question=question,\n",
    "                documents=documents,\n",
    "                draft_answer=\"\",\n",
    "                verification_report=\"\",\n",
    "                is_relevant=False,\n",
    "                retriever=retriever\n",
    "            )\n",
    "            \n",
    "            final_state = self.compiled_workflow.invoke(initial_state)\n",
    "            \n",
    "            return {\n",
    "                \"draft_answer\": final_state[\"draft_answer\"],\n",
    "                \"verification_report\": final_state[\"verification_report\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Workflow execution failed: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Process Function: Tying It All Together\n",
    "\n",
    "The `process_documents_and_answer_questions` function serves as the **main entry point** for the entire RAG system. It orchestrates the end-to-end process from document processing to question answering.\n",
    "\n",
    "#### Function Overview\n",
    "\n",
    "This function takes a list of document file paths and questions as input, then returns answers with verification reports. It seamlessly integrates all components of the system:\n",
    "- Document processing and chunking\n",
    "- Hybrid retrieval system setup\n",
    "- Multi-agent workflow execution\n",
    "- Error handling and reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def process_documents_and_answer_questions(file_paths: List[str], questions: List[str]) -> List[Dict]:\n",
    "    \"\"\"Process documents and answer a list of questions\"\"\"\n",
    "    # Initialize components\n",
    "    processor = DocumentProcessor()\n",
    "    retriever_builder = RetrieverBuilder()\n",
    "    workflow = AgentWorkflow()\n",
    "    \n",
    "    try:\n",
    "        # Process documents\n",
    "        logger.info(f\"Processing documents: {file_paths}\")\n",
    "        \n",
    "        # Convert file paths to File-like objects expected by the processor\n",
    "        class SimpleFile:\n",
    "            def __init__(self, path):\n",
    "                self.name = path\n",
    "        \n",
    "        files = [SimpleFile(path) for path in file_paths]\n",
    "        \n",
    "        # Process files into chunks\n",
    "        chunks = processor.process(files)\n",
    "        logger.info(f\"Generated {len(chunks)} document chunks\")\n",
    "        \n",
    "        # Build retriever\n",
    "        retriever = retriever_builder.build_hybrid_retriever(chunks)\n",
    "        \n",
    "        # Process each question\n",
    "        results = []\n",
    "        for i, question in enumerate(questions):\n",
    "            logger.info(f\"Processing question {i+1}/{len(questions)}: {question}\")\n",
    "            \n",
    "            # Run the workflow\n",
    "            result = workflow.full_pipeline(question, retriever)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": result[\"draft_answer\"],\n",
    "                \"verification\": result[\"verification_report\"]\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing documents: {e}\")\n",
    "        return [{\n",
    "            \"question\": q,\n",
    "            \"answer\": f\"Error: {str(e)}\",\n",
    "            \"verification\": \"\"\n",
    "        } for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage: Running the RAG System\n",
    "\n",
    "The example code demonstrates how to use the document question-answering system in a script or notebook context. This serves as both a practical example for users and a testing mechanism for the system.\n",
    "\n",
    "### How the Example Works\n",
    "\n",
    "1. **Document and Question Definition**:\n",
    "  - Specifies file paths to documents that will be processed\n",
    "  - Defines a list of questions to be answered about those documents\n",
    "  - In this example, it uses Google's 2024 Environmental Report as the document source\n",
    "\n",
    "2. **System Execution**:\n",
    "  - Calls the main `process_documents_and_answer_questions` function\n",
    "  - Passes the documents and questions as parameters\n",
    "  - The function handles all processing and returns structured results\n",
    "\n",
    "3. **Result Presentation**:\n",
    "  - Formats and displays the results in a clear, readable format\n",
    "  - For each question, it shows:\n",
    "    - The original question\n",
    "    - The generated answer\n",
    "    - The verification report with fact-checking information\n",
    "  - Uses visual separators to distinguish between different questions\n",
    "\n",
    "### Usage Flexibility\n",
    "\n",
    "This pattern can be easily modified for various use cases:\n",
    "- Processing multiple documents simultaneously\n",
    "- Asking domain-specific questions about technical content\n",
    "- Integrating into larger applications or workflows\n",
    "- Running in batch mode for document analysis\n",
    "\n",
    "The simplicity of the interface belies the sophisticated multi-agent system working underneath, delivering a user-friendly experience for document-based question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:02:50,873 - ibm_watsonx_ai.client - INFO - Client successfully initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResearchAgent with IBM WatsonX ModelInference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:02:51,301 - ibm_watsonx_ai.client - INFO - Client successfully initialized\n",
      "2025-05-12 16:02:52,338 - httpx - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-04-23&project_id=skills-network&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:02:52,344 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-04-23&project_id=skills-network&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInference initialized successfully.\n",
      "Initializing VerificationAgent with IBM WatsonX ModelInference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:02:52,576 - ibm_watsonx_ai.client - INFO - Client successfully initialized\n",
      "2025-05-12 16:02:53,247 - httpx - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-04-23&project_id=skills-network&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:02:53,260 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-04-23&project_id=skills-network&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-05-12 16:02:53,399 - ibm_watsonx_ai.client - INFO - Client successfully initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInference initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:02:53,859 - httpx - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-04-23&project_id=skills-network&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:02:53,870 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-04-23&project_id=skills-network&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-05-12 16:02:53,890 - __main__ - INFO - Processing documents: ['Pandas Cheat Sheet.pdf']\n",
      "2025-05-12 16:02:53,946 - __main__ - INFO - Processing and caching: Pandas Cheat Sheet.pdf\n",
      "2025-05-12 16:02:53,984 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-05-12 16:02:53,985 - docling.document_converter - INFO - Initializing pipeline for StandardPdfPipeline with options hash 70041f74270850b7bedf7c8f5c2dcede\n",
      "2025-05-12 16:02:54,026 - docling.models.factories.base_factory - INFO - Loading plugin 'docling_defaults'\n",
      "2025-05-12 16:02:54,027 - docling.models.factories - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-05-12 16:02:54,085 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cpu'\n",
      "2025-05-12 16:02:54,088 - easyocr.easyocr - WARNING - Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
      "2025-05-12 16:02:55,199 - easyocr.easyocr - INFO - Download complete\n",
      "2025-05-12 16:02:55,200 - easyocr.easyocr - WARNING - Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n",
      "2025-05-12 16:02:55,492 - easyocr.easyocr - INFO - Download complete.\n",
      "2025-05-12 16:02:58,278 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cpu'\n",
      "2025-05-12 16:03:19,076 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cpu'\n",
      "2025-05-12 16:03:20,125 - docling.models.factories.base_factory - INFO - Loading plugin 'docling_defaults'\n",
      "2025-05-12 16:03:20,127 - docling.models.factories - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-05-12 16:03:20,128 - docling.pipeline.base_pipeline - INFO - Processing document Pandas Cheat Sheet.pdf\n",
      "2025-05-12 16:09:52,984 - docling.document_converter - INFO - Finished converting document Pandas Cheat Sheet.pdf in 419.03 sec.\n",
      "2025-05-12 16:09:53,202 - __main__ - INFO - Total unique chunks: 42\n",
      "2025-05-12 16:09:53,203 - __main__ - INFO - Generated 42 document chunks\n",
      "2025-05-12 16:09:53,207 - __main__ - INFO - BM25 retriever created successfully.\n",
      "2025-05-12 16:09:56,494 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:09:56,499 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23'\n",
      "2025-05-12 16:09:56,554 - __main__ - INFO - Vector store created successfully.\n",
      "2025-05-12 16:09:56,555 - __main__ - INFO - Vector retriever created successfully.\n",
      "2025-05-12 16:09:56,556 - __main__ - INFO - Hybrid retriever created successfully.\n",
      "2025-05-12 16:09:56,556 - __main__ - INFO - Processing question 1/2: What is the main topic discussed in the document?\n",
      "2025-05-12 16:09:56,687 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:09:56,689 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23'\n",
      "2025-05-12 16:09:56,699 - __main__ - INFO - Retrieved 9 relevant documents (from .invoke)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting full_pipeline with question='What is the main topic discussed in the document?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:09:56,828 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:09:56,830 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23'\n",
      "2025-05-12 16:09:57,120 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:09:57,121 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished chat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checker response: CAN_ANSWER\n",
      "[DEBUG] _decide_after_relevance_check -> relevant\n",
      "[DEBUG] Entered _research_step with question='What is the main topic discussed in the document?'\n",
      "ResearchAgent.generate called with question='What is the main topic discussed in the document?' and 9 documents.\n",
      "Combined context length: 6255 characters.\n",
      "Prompt created for the LLM.\n",
      "Sending prompt to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:10:03,976 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:03,979 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished chat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response received.\n",
      "Raw LLM response:\n",
      "The main topic discussed in the document is the Pandas library in Python, specifically its data structures and tools for data manipulation and analysis, with a focus on DataFrames and various operations that can be performed on them.\n",
      "Generated answer: The main topic discussed in the document is the Pandas library in Python, specifically its data structures and tools for data manipulation and analysis, with a focus on DataFrames and various operations that can be performed on them.\n",
      "[DEBUG] Researcher returned draft answer.\n",
      "[DEBUG] Entered _verification_step. Verifying the draft answer...\n",
      "VerificationAgent.check called with answer='The main topic discussed in the document is the Pandas library in Python, specifically its data structures and tools for data manipulation and analysis, with a focus on DataFrames and various operations that can be performed on them.' and 9 documents.\n",
      "Combined context length: 6255 characters.\n",
      "Prompt created for the LLM.\n",
      "Sending prompt to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:10:05,980 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:05,981 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished chat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23'\n",
      "2025-05-12 16:10:05,982 - __main__ - INFO - [DEBUG] Verification successful, ending workflow.\n",
      "2025-05-12 16:10:05,983 - __main__ - INFO - Workflow reached end step\n",
      "2025-05-12 16:10:05,984 - __main__ - INFO - Processing question 2/2: Summarize the key findings in the document.\n",
      "2025-05-12 16:10:06,148 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:06,150 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23'\n",
      "2025-05-12 16:10:06,156 - __main__ - INFO - Retrieved 9 relevant documents (from .invoke)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response received.\n",
      "Raw LLM response:\n",
      "Supported: YES\n",
      "Unsupported Claims: None\n",
      "Contradictions: None\n",
      "Relevant: YES\n",
      "Additional Details: The answer accurately reflects the main topic of the provided context, which is the Pandas library in Python, focusing on its data structures, specifically DataFrames, and various operations that can be performed on them. The answer covers selecting rows and columns, calculating descriptive statistics, reshaping DataFrames using stack and unstack, applying functions to columns and entire DataFrames, and converting between wide and long formats using melt and pivot functions. The answer also includes code examples demonstrating these operations.\n",
      "Verification report:\n",
      "**Supported:** YES\n",
      "**Unsupported Claims:** None\n",
      "**Contradictions:** None\n",
      "**Relevant:** YES\n",
      "**Additional Details:** None\n",
      "\n",
      "Context used: Pandas is a powerful, open-source Python library for data manipulation and analysis. It provides data structures for efficiently storing large datasets and tools for working with them. The primary data structure in Pandas is the DataFrame , a two-dimensional table with labeled rows and columns, similar to a spreadsheet or SQL table.\n",
      "\n",
      "```\n",
      "# Select the first row (index position 0) first_row = df.iloc[0] print(first_row)\n",
      "```  \n",
      "```\n",
      "col1       1 col2       a col3    True\n",
      "```  \n",
      "```\n",
      "Name: 0, dtype: object\n",
      "```  \n",
      "```\n",
      "# Select the first two rows first_two_rows = df.iloc[:2] print(first_two_rows)\n",
      "```\n",
      "\n",
      "| Function       | Description                                                                           |\n",
      "|----------------|---------------------------------------------------------------------------------------|\n",
      "| df.mean()      | Calculates the mean of each column.                                                   |\n",
      "| df.median()    | Calculates the median of each column.                                                 |\n",
      "| df.min()       | Finds the minimum value in each column.                                               |\n",
      "| df.max()       | Finds the maximum value in each column.                                               |\n",
      "| df.std()       | Calculates the standard deviation of each column.                                     |\n",
      "| df.sum()       | Calculates the sum of each column.                                                    |\n",
      "| df.count()     | Counts the number of non-missing values in each column.                               |\n",
      "| df.quantile(q) | Calculates the q-th quantile (e.g., 0.25 for the first quartile, 0.5 for the median). |\n",
      "\n",
      "```\n",
      "# Select row with index label 1, and columns 'col2' and 'col3' subset = df.loc[1, ['col2', 'col3']] print(subset)\n",
      "```  \n",
      "```\n",
      "col2        b col3    False Name: 1, dtype: object\n",
      "```\n",
      "\n",
      "| Operation        | Code       | Description                               |\n",
      "|------------------|------------|-------------------------------------------|\n",
      "| View top rows    | df.head(n) | Displays the first n rows (default is 5). |\n",
      "| View bottom rows | df.tail(n) | Displays the last n rows (default is 5).  |  \n",
      "| Operation         | Code          | Description                                                                    |\n",
      "|-------------------|---------------|--------------------------------------------------------------------------------|\n",
      "| DataFrame info    | df.info()     | Provides a summary of the DataFrame, including data types and non-null values. |\n",
      "| Descriptive stats | df.describe() | Generates descriptive statistics (e.g., mean, std, min, max).                  |\n",
      "| Shape             | df.shape      | Returns the dimensions (rows, columns) of the DataFrame.                       |\n",
      "| Columns           | df.columns    | Returns the column labels of the DataFrame.                                    |\n",
      "| Index             | df.index      | Returns the index (row labels) of the DataFrame.                               |\n",
      "| Data types        | df.dtypes     | Returns the data types of each column.                                         |\n",
      "\n",
      "```\n",
      "# Set the value of 'col2' at index label 0 to 'z' df.loc[0, 'col2'] = 'z' print(df)\n",
      "```  \n",
      "|    |   col1 | col2   | col3   |\n",
      "|----|--------|--------|--------|\n",
      "|  0 |      1 | z      | True   |\n",
      "|  1 |      2 | b      | False  |\n",
      "|  2 |      3 | c      | True   |\n",
      "\n",
      "stack() : Reshapes a DataFrame by stacking the specified level(s) from columns to index, making the DataFrame taller. unstack() : Performs the inverse operation of stack, moving levels from the index to the columns, making the DataFrame wider.  \n",
      "```\n",
      "# Assuming df_wide_again is already stacked df_stacked = df_wide_again.stack() print(\"Stacked DataFrame:\\n\", df_stacked) # Unstacking the DataFrame df_unstacked = df_stacked.unstack() print(\"\\nUnstacked DataFrame:\\n\", df_unstacked)\n",
      "```  \n",
      "```\n",
      "Stacked DataFrame: Name   Subject Alice  English    78.0 Math       80.0 Science    85.0 Bob    English    88.0 Math       90.0 Science    92.0 dtype: float64 Unstacked DataFrame: Subject  English  Math  Science Name Alice       78.0  80.0     85.0 Bob         88.0  90.0     92.0\n",
      "```\n",
      "\n",
      "```\n",
      "# Apply a function to each element in a column df['A_squared'] = df['A'].apply(lambda x: x**2) print(df)\n",
      "```  \n",
      "|    |   A |   B |   C |   A_squared |\n",
      "|----|-----|-----|-----|-------------|\n",
      "|  0 |   1 |   4 |   7 |           1 |\n",
      "|  1 |   2 | nan |   8 |           4 |  \n",
      "|    |   A |   B C |    |   A_squared |\n",
      "|----|-----|-------|----|-------------|\n",
      "|  2 | nan |     6 |  9 |         nan |  \n",
      "```\n",
      "# Apply a function to the entire DataFrame def add_one(x): return x + 1 df = df.apply(add_one) print(df)\n",
      "```  \n",
      "|    |   A |   B |   C |   A_squared |\n",
      "|----|-----|-----|-----|-------------|\n",
      "|  0 |   2 |   5 |   8 |           2 |\n",
      "|  1 |   3 | nan |   9 |           5 |\n",
      "|  2 | nan |   7 |  10 |         nan |\n",
      "\n",
      "```\n",
      "1. melt() (Unpivot)\n",
      "```  \n",
      "Converts a \"wide\" DataFrame to a \"long\" format.  \n",
      "```\n",
      "# Create a wide DataFrame data = {'Name': ['Alice', 'Bob'], 'Math': [80, 90], 'Science': [85, 92], 'English': [78, 88]} df_wide = pd.DataFrame(data) print(df_wide)\n",
      "```  \n",
      "|    | Name   |   Math |   Science |   English |\n",
      "|----|--------|--------|-----------|-----------|\n",
      "|  0 | Alice  |     80 |        85 |        78 |\n",
      "|  1 | Bob    |     90 |        92 |        88 |  \n",
      "```\n",
      "# Melt the DataFrame to make it long df_long = df_wide.melt(id_vars=['Name'], var_name='Subject', value_name='Score') print(df_long)\n",
      "```  \n",
      "|    | Name   | Subject   |   Score |\n",
      "|----|--------|-----------|---------|\n",
      "|  0 | Alice  | Math      |      80 |\n",
      "|  1 | Bob    | Math      |      90 |\n",
      "|  2 | Alice  | Science   |      85 |\n",
      "|  3 | Bob    | Science   |      92 |\n",
      "|  4 | Alice  | English   |      78 |\n",
      "|  5 | Bob    | English   |      88 |  \n",
      "<!-- formula-not-decoded -->  \n",
      "Converts a \"long\" DataFrame to a \"wide\" format (opposite of melt() ).  \n",
      "```\n",
      "# Pivot the long DataFrame back to wide format df_wide_again = df_long.pivot(index='Name', columns='Subject', values='Score') print(df_wide_again)\n",
      "```  \n",
      "Subject English Math Science  \n",
      "Name  \n",
      "| Subject   |   English |   Math |   Science |\n",
      "|-----------|-----------|--------|-----------|\n",
      "| Alice     |        78 |     80 |        85 |\n",
      "| Bob       |        88 |     90 |        92 |\n",
      "[DEBUG] VerificationAgent returned a verification report.\n",
      "[DEBUG] _decide_next_step with verification_report='**Supported:** YES\n",
      "**Unsupported Claims:** None\n",
      "**Contradictions:** None\n",
      "**Relevant:** YES\n",
      "**Additional Details:** None\n",
      "'\n",
      "[DEBUG] Starting full_pipeline with question='Summarize the key findings in the document.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:10:06,295 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:06,296 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-04-23'\n",
      "2025-05-12 16:10:06,789 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:06,791 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished chat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checker response: PARTIAL\n",
      "[DEBUG] _decide_after_relevance_check -> relevant\n",
      "[DEBUG] Entered _research_step with question='Summarize the key findings in the document.'\n",
      "ResearchAgent.generate called with question='Summarize the key findings in the document.' and 9 documents.\n",
      "Combined context length: 5014 characters.\n",
      "Prompt created for the LLM.\n",
      "Sending prompt to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:10:38,557 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:38,558 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished chat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response received.\n",
      "Raw LLM response:\n",
      "There are no specific key findings in the provided document as it appears to be a reference guide or documentation for Pandas, a Python library for data manipulation and analysis. The document provides an overview of various Pandas functions and operations, including data frame creation, merging, concatenation, and data manipulation.\n",
      "\n",
      "However, some key points that can be summarized from the document are:\n",
      "\n",
      "1. Pandas provides various functions for data frame creation, including `df.mean()`, `df.median()`, `df.min()`, `df.max()`, `df.std()`, `df.sum()`, `df.count()`, and `df.quantile(q)`.\n",
      "\n",
      "2. Data frames can be merged based on common columns using `pd.merge()` with different merge types, including inner, left, right, and outer merges.\n",
      "\n",
      "3. Data frames can be concatenated along rows (vertically) or columns (horizontally) using `pd.concat()`.\n",
      "\n",
      "4. Pandas provides various functions for data frame manipulation, including `df.head(n)`, `df.tail(n)`, `df.info()`, `df.describe()`, `df.shape`, `df.columns`, `df.index`, and `df.dtypes`.\n",
      "\n",
      "5. Data frames can be subsetted using `df.loc[]` and `df.iloc[]`, and values can be set using `df.loc[]` and `df.iloc[]`.\n",
      "\n",
      "Overall, the document provides a comprehensive overview of Pandas functions and operations, which can be useful for data manipulation and\n",
      "Generated answer: There are no specific key findings in the provided document as it appears to be a reference guide or documentation for Pandas, a Python library for data manipulation and analysis. The document provides an overview of various Pandas functions and operations, including data frame creation, merging, concatenation, and data manipulation.\n",
      "\n",
      "However, some key points that can be summarized from the document are:\n",
      "\n",
      "1. Pandas provides various functions for data frame creation, including `df.mean()`, `df.median()`, `df.min()`, `df.max()`, `df.std()`, `df.sum()`, `df.count()`, and `df.quantile(q)`.\n",
      "\n",
      "2. Data frames can be merged based on common columns using `pd.merge()` with different merge types, including inner, left, right, and outer merges.\n",
      "\n",
      "3. Data frames can be concatenated along rows (vertically) or columns (horizontally) using `pd.concat()`.\n",
      "\n",
      "4. Pandas provides various functions for data frame manipulation, including `df.head(n)`, `df.tail(n)`, `df.info()`, `df.describe()`, `df.shape`, `df.columns`, `df.index`, and `df.dtypes`.\n",
      "\n",
      "5. Data frames can be subsetted using `df.loc[]` and `df.iloc[]`, and values can be set using `df.loc[]` and `df.iloc[]`.\n",
      "\n",
      "Overall, the document provides a comprehensive overview of Pandas functions and operations, which can be useful for data manipulation and\n",
      "[DEBUG] Researcher returned draft answer.\n",
      "[DEBUG] Entered _verification_step. Verifying the draft answer...\n",
      "VerificationAgent.check called with answer='There are no specific key findings in the provided document as it appears to be a reference guide or documentation for Pandas, a Python library for data manipulation and analysis. The document provides an overview of various Pandas functions and operations, including data frame creation, merging, concatenation, and data manipulation.\n",
      "\n",
      "However, some key points that can be summarized from the document are:\n",
      "\n",
      "1. Pandas provides various functions for data frame creation, including `df.mean()`, `df.median()`, `df.min()`, `df.max()`, `df.std()`, `df.sum()`, `df.count()`, and `df.quantile(q)`.\n",
      "\n",
      "2. Data frames can be merged based on common columns using `pd.merge()` with different merge types, including inner, left, right, and outer merges.\n",
      "\n",
      "3. Data frames can be concatenated along rows (vertically) or columns (horizontally) using `pd.concat()`.\n",
      "\n",
      "4. Pandas provides various functions for data frame manipulation, including `df.head(n)`, `df.tail(n)`, `df.info()`, `df.describe()`, `df.shape`, `df.columns`, `df.index`, and `df.dtypes`.\n",
      "\n",
      "5. Data frames can be subsetted using `df.loc[]` and `df.iloc[]`, and values can be set using `df.loc[]` and `df.iloc[]`.\n",
      "\n",
      "Overall, the document provides a comprehensive overview of Pandas functions and operations, which can be useful for data manipulation and' and 9 documents.\n",
      "Combined context length: 5014 characters.\n",
      "Prompt created for the LLM.\n",
      "Sending prompt to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:10:39,829 - httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23 \"HTTP/1.1 200 OK\"\n",
      "2025-05-12 16:10:39,830 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished chat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-04-23'\n",
      "2025-05-12 16:10:39,831 - __main__ - INFO - [DEBUG] Verification successful, ending workflow.\n",
      "2025-05-12 16:10:39,832 - __main__ - INFO - Workflow reached end step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response received.\n",
      "Raw LLM response:\n",
      "Supported: YES\n",
      "Unsupported Claims: None\n",
      "Contradictions: None\n",
      "Relevant: YES\n",
      "Additional Details: The provided answer accurately summarizes the key functions and operations of the Pandas library for data manipulation and analysis, as described in the context. It covers data frame creation, merging, concatenation, and data manipulation functions, providing direct factual support for each point. The answer is highly relevant to the question, as it directly addresses the functions and operations of the Pandas library.\n",
      "Verification report:\n",
      "**Supported:** YES\n",
      "**Unsupported Claims:** None\n",
      "**Contradictions:** None\n",
      "**Relevant:** YES\n",
      "**Additional Details:** None\n",
      "\n",
      "Context used: | Function       | Description                                                                           |\n",
      "|----------------|---------------------------------------------------------------------------------------|\n",
      "| df.mean()      | Calculates the mean of each column.                                                   |\n",
      "| df.median()    | Calculates the median of each column.                                                 |\n",
      "| df.min()       | Finds the minimum value in each column.                                               |\n",
      "| df.max()       | Finds the maximum value in each column.                                               |\n",
      "| df.std()       | Calculates the standard deviation of each column.                                     |\n",
      "| df.sum()       | Calculates the sum of each column.                                                    |\n",
      "| df.count()     | Counts the number of non-missing values in each column.                               |\n",
      "| df.quantile(q) | Calculates the q-th quantile (e.g., 0.25 for the first quartile, 0.5 for the median). |\n",
      "\n",
      "Merges DataFrames based on common columns (like SQL joins).  \n",
      "```\n",
      "# Create two DataFrames with a common column left = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'A': ['A0', 'A1', 'A2']}) right = pd.DataFrame({'key': ['K0', 'K1', 'K3'], 'B': ['B0', 'B1', 'B3']}) print(left) print(right)\n",
      "```\n",
      "\n",
      "0  \n",
      "1  \n",
      "2  \n",
      "0  \n",
      "1  \n",
      "2  \n",
      "key  \n",
      "K0  \n",
      "K1  \n",
      "K2  \n",
      "key  \n",
      "K0  \n",
      "K1  \n",
      "A  \n",
      "A0  \n",
      "A1  \n",
      "A2  \n",
      "B  \n",
      "B0  \n",
      "B1  \n",
      "K3  \n",
      "B3  \n",
      "```\n",
      "# Inner merge (only common keys) df_inner = pd.merge(left, right, on='key', how='inner') print(df_inner)\n",
      "```  \n",
      "|    | key   | A   | B   |\n",
      "|----|-------|-----|-----|\n",
      "|  0 | K0    | A0  | B0  |\n",
      "|  1 | K1    | A1  | B1  |  \n",
      "```\n",
      "# Left merge (all keys from left DataFrame) df_left = pd.merge(left, right, on='key', how='left') print(df_left)\n",
      "```  \n",
      "|    | key   | A   | B   |\n",
      "|----|-------|-----|-----|\n",
      "|  0 | K0    | A0  | B0  |\n",
      "|  1 | K1    | A1  | B1  |\n",
      "|  2 | K2    | A2  | NaN |  \n",
      "```\n",
      "# Right merge (all keys from right DataFrame) df_right = pd.merge(left, right, on='key', how='right') print(df_right)\n",
      "```  \n",
      "|    | key   | A   | B   |\n",
      "|----|-------|-----|-----|\n",
      "|  0 | K0    | A0  | B0  |\n",
      "|  1 | K1    | A1  | B1  |\n",
      "|  2 | K3    | NaN | B3  |  \n",
      "```\n",
      "# Outer merge (all keys from both DataFrames) df_outer = pd.merge(left, right, on='key', how='outer') print(df_outer)\n",
      "```  \n",
      "|    | key   | A   | B   |\n",
      "|----|-------|-----|-----|\n",
      "|  0 | K0    | A0  | B0  |\n",
      "|  1 | K1    | A1  | B1  |\n",
      "|  2 | K2    | A2  | NaN |\n",
      "|  3 | K3    | NaN | B3  |\n",
      "\n",
      "Concatenates DataFrames along rows (vertically) or columns (horizontally).  \n",
      "```\n",
      "# Create two DataFrames df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']}) df2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']}) print(df1) print(df2)\n",
      "```\n",
      "\n",
      "Pandas is a powerful, open-source Python library for data manipulation and analysis. It provides data structures for efficiently storing large datasets and tools for working with them. The primary data structure in Pandas is the DataFrame , a two-dimensional table with labeled rows and columns, similar to a spreadsheet or SQL table.\n",
      "\n",
      "```\n",
      "# Returns a DataFrame subset_df = df[['col1', 'col3']] print(subset_df)\n",
      "```  \n",
      "|    |   col1 | col3   |\n",
      "|----|--------|--------|\n",
      "|  0 |      1 | True   |\n",
      "|  1 |      2 | False  |\n",
      "|  2 |      3 | True   |\n",
      "\n",
      "| Operation        | Code       | Description                               |\n",
      "|------------------|------------|-------------------------------------------|\n",
      "| View top rows    | df.head(n) | Displays the first n rows (default is 5). |\n",
      "| View bottom rows | df.tail(n) | Displays the last n rows (default is 5).  |  \n",
      "| Operation         | Code          | Description                                                                    |\n",
      "|-------------------|---------------|--------------------------------------------------------------------------------|\n",
      "| DataFrame info    | df.info()     | Provides a summary of the DataFrame, including data types and non-null values. |\n",
      "| Descriptive stats | df.describe() | Generates descriptive statistics (e.g., mean, std, min, max).                  |\n",
      "| Shape             | df.shape      | Returns the dimensions (rows, columns) of the DataFrame.                       |\n",
      "| Columns           | df.columns    | Returns the column labels of the DataFrame.                                    |\n",
      "| Index             | df.index      | Returns the index (row labels) of the DataFrame.                               |\n",
      "| Data types        | df.dtypes     | Returns the data types of each column.                                         |\n",
      "\n",
      "```\n",
      "# Select row with index label 1, and columns 'col2' and 'col3' subset = df.loc[1, ['col2', 'col3']] print(subset)\n",
      "```  \n",
      "```\n",
      "col2        b col3    False Name: 1, dtype: object\n",
      "```\n",
      "\n",
      "```\n",
      "# Set the value of 'col2' at index label 0 to 'z' df.loc[0, 'col2'] = 'z' print(df)\n",
      "```  \n",
      "|    |   col1 | col2   | col3   |\n",
      "|----|--------|--------|--------|\n",
      "|  0 |      1 | z      | True   |\n",
      "|  1 |      2 | b      | False  |\n",
      "|  2 |      3 | c      | True   |\n",
      "[DEBUG] VerificationAgent returned a verification report.\n",
      "[DEBUG] _decide_next_step with verification_report='**Supported:** YES\n",
      "**Unsupported Claims:** None\n",
      "**Contradictions:** None\n",
      "**Relevant:** YES\n",
      "**Additional Details:** None\n",
      "'\n",
      "\n",
      "===== Question 1 =====\n",
      "Q: What is the main topic discussed in the document?\n",
      "\n",
      "Answer: The main topic discussed in the document is the Pandas library in Python, specifically its data structures and tools for data manipulation and analysis, with a focus on DataFrames and various operations that can be performed on them.\n",
      "\n",
      "Verification: **Supported:** YES\n",
      "**Unsupported Claims:** None\n",
      "**Contradictions:** None\n",
      "**Relevant:** YES\n",
      "**Additional Details:** None\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Question 2 =====\n",
      "Q: Summarize the key findings in the document.\n",
      "\n",
      "Answer: There are no specific key findings in the provided document as it appears to be a reference guide or documentation for Pandas, a Python library for data manipulation and analysis. The document provides an overview of various Pandas functions and operations, including data frame creation, merging, concatenation, and data manipulation.\n",
      "\n",
      "However, some key points that can be summarized from the document are:\n",
      "\n",
      "1. Pandas provides various functions for data frame creation, including `df.mean()`, `df.median()`, `df.min()`, `df.max()`, `df.std()`, `df.sum()`, `df.count()`, and `df.quantile(q)`.\n",
      "\n",
      "2. Data frames can be merged based on common columns using `pd.merge()` with different merge types, including inner, left, right, and outer merges.\n",
      "\n",
      "3. Data frames can be concatenated along rows (vertically) or columns (horizontally) using `pd.concat()`.\n",
      "\n",
      "4. Pandas provides various functions for data frame manipulation, including `df.head(n)`, `df.tail(n)`, `df.info()`, `df.describe()`, `df.shape`, `df.columns`, `df.index`, and `df.dtypes`.\n",
      "\n",
      "5. Data frames can be subsetted using `df.loc[]` and `df.iloc[]`, and values can be set using `df.loc[]` and `df.iloc[]`.\n",
      "\n",
      "Overall, the document provides a comprehensive overview of Pandas functions and operations, which can be useful for data manipulation and\n",
      "\n",
      "Verification: **Supported:** YES\n",
      "**Unsupported Claims:** None\n",
      "**Contradictions:** None\n",
      "**Relevant:** YES\n",
      "**Additional Details:** None\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example documents and questions\n",
    "    documents = [\"Pandas Cheat Sheet.pdf\"]\n",
    "    questions = [\n",
    "        \"What is the main topic discussed in the document?\",\n",
    "        \"Summarize the key findings in the document.\"\n",
    "    ]\n",
    "    \n",
    "    # Process documents and get answers\n",
    "    results = process_documents_and_answer_questions(documents, questions)\n",
    "    \n",
    "    # Print results\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n===== Question {i+1} =====\")\n",
    "        print(f\"Q: {result['question']}\")\n",
    "        print(f\"\\nAnswer: {result['answer']}\")\n",
    "        print(f\"\\nVerification: {result['verification']}\")\n",
    "        print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "f00c70fa4e66aee7228e4b041789f817ae54903b2201c032a27e616af7699650"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
